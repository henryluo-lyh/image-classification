{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dl8KFK4T8ewZ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import Caltech101\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AOGLK1EHn28y",
    "outputId": "ba3012ce-fc5e-4e6c-d737-63f46233c340"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.data import Subset, random_split\n",
    "\n",
    "root = './data'\n",
    "\n",
    "caltech101 = Caltech101(root, download=True)\n",
    "\n",
    "# Define the custom transform to convert grayscale images to RGB\n",
    "def grayscale_to_rgb(image):\n",
    "    if image.mode == 'L':  \n",
    "        image = image.convert('RGB') \n",
    "    return image\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    Lambda(grayscale_to_rgb),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomCrop(224, padding=4),\n",
    "    transforms.RandomRotation(degrees=(-10, 10)),\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "      Lambda(grayscale_to_rgb),\n",
    "      transforms.Resize((224, 224)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "      Lambda(grayscale_to_rgb),\n",
    "      transforms.Resize((224, 224)),\n",
    "      transforms.ToTensor(),\n",
    "      transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# caltech101 = Caltech101(root, transform = test_transform, download=True)\n",
    "\n",
    "# Split the dataset into train and test sets\n",
    "train_size = int(0.7 * len(caltech101))\n",
    "validation_size = int(0.15 * len(caltech101))\n",
    "test_size = test_size = len(caltech101) - train_size - validation_size\n",
    "\n",
    "train_dataset, validation_dataset, test_dataset = torch.utils.data.random_split(caltech101, [train_size,validation_size, test_size])\n",
    "#train_dataset, test_dataset = torch.utils.data.random_split(caltech101, [train_size, test_size])\n",
    "# apply transformation to the data\n",
    "class TransformedSubset(Subset):\n",
    "    def __init__(self, dataset, indices, transform=None):\n",
    "        super(TransformedSubset, self).__init__(dataset, indices)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.dataset[self.indices[idx]]\n",
    "        if self.transform:\n",
    "            x = self.transform(x)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# apply transformation to the data\n",
    "train_dataset = TransformedSubset(train_dataset.dataset, train_dataset.indices, transform=train_transform)\n",
    "validation_dataset = TransformedSubset(validation_dataset.dataset, validation_dataset.indices, transform=validation_transform)\n",
    "test_dataset = TransformedSubset(test_dataset.dataset, test_dataset.indices, transform=test_transform)\n",
    "\n",
    "\n",
    "# Define the data loaders for the train and test sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tQTrZCFjxsN6"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def test(model, test_loader, device):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    print(\"accuracy:\", accuracy, \"f1:\", f1, \"precision\", precision, \"recall\", recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFXIQMyf2I_4"
   },
   "outputs": [],
   "source": [
    "# ResNet implementation learned from:\n",
    "# https://niko-gamulin.medium.com/resnet-implementation-with-pytorch-from-scratch-23cf3047cb93\n",
    "# https://blog.paperspace.com/writing-resnet-from-scratch-in-pytorch/\n",
    "\n",
    "import torch.nn.functional as F\n",
    "# Define the building blocks for ResNet\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "# Define the ResNet architecture\n",
    "\n",
    "\n",
    "# (continuing) Define the ResNet architecture\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=101):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def resnet50():\n",
    "    return ResNet(Bottleneck, [3, 4, 6, 3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 807
    },
    "id": "pzK-Ntj23CXa",
    "outputId": "83916be9-12f5-4efe-e869-8f33e811f572"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Train Loss: 4.2415, Validation Loss: 4.0377, Validation Accuracy: 0.1514\n",
      "Epoch 2/100, Train Loss: 4.0487, Validation Loss: 3.9359, Validation Accuracy: 0.1906\n",
      "Epoch 3/100, Train Loss: 3.9524, Validation Loss: 3.8725, Validation Accuracy: 0.1914\n",
      "Epoch 4/100, Train Loss: 3.8674, Validation Loss: 3.8293, Validation Accuracy: 0.2183\n",
      "Epoch 5/100, Train Loss: 3.7978, Validation Loss: 3.7633, Validation Accuracy: 0.2321\n",
      "Epoch 6/100, Train Loss: 3.7287, Validation Loss: 3.6830, Validation Accuracy: 0.2452\n",
      "Epoch 7/100, Train Loss: 3.6405, Validation Loss: 3.5773, Validation Accuracy: 0.2613\n",
      "Epoch 8/100, Train Loss: 3.5536, Validation Loss: 3.4401, Validation Accuracy: 0.2729\n",
      "Epoch 9/100, Train Loss: 3.4700, Validation Loss: 3.3841, Validation Accuracy: 0.2806\n",
      "Epoch 10/100, Train Loss: 3.3913, Validation Loss: 3.2829, Validation Accuracy: 0.2867\n",
      "Epoch 11/100, Train Loss: 3.3147, Validation Loss: 3.1974, Validation Accuracy: 0.2936\n",
      "Epoch 12/100, Train Loss: 3.2490, Validation Loss: 3.1103, Validation Accuracy: 0.3236\n",
      "Epoch 13/100, Train Loss: 3.1877, Validation Loss: 3.0617, Validation Accuracy: 0.3182\n",
      "Epoch 14/100, Train Loss: 3.1199, Validation Loss: 3.0240, Validation Accuracy: 0.3397\n",
      "Epoch 15/100, Train Loss: 3.0765, Validation Loss: 2.9491, Validation Accuracy: 0.3428\n",
      "Epoch 16/100, Train Loss: 3.0248, Validation Loss: 2.9242, Validation Accuracy: 0.3451\n",
      "Epoch 17/100, Train Loss: 2.9618, Validation Loss: 2.8570, Validation Accuracy: 0.3528\n",
      "Epoch 18/100, Train Loss: 2.9162, Validation Loss: 2.8050, Validation Accuracy: 0.3782\n",
      "Epoch 19/100, Train Loss: 2.8483, Validation Loss: 2.7809, Validation Accuracy: 0.3720\n",
      "Epoch 20/100, Train Loss: 2.8135, Validation Loss: 2.7225, Validation Accuracy: 0.3866\n",
      "Epoch 21/100, Train Loss: 2.7604, Validation Loss: 2.6888, Validation Accuracy: 0.3912\n",
      "Epoch 22/100, Train Loss: 2.7187, Validation Loss: 2.6419, Validation Accuracy: 0.4051\n",
      "Epoch 23/100, Train Loss: 2.6829, Validation Loss: 2.6001, Validation Accuracy: 0.4143\n",
      "Epoch 24/100, Train Loss: 2.6259, Validation Loss: 2.5606, Validation Accuracy: 0.4097\n",
      "Epoch 25/100, Train Loss: 2.5819, Validation Loss: 2.5191, Validation Accuracy: 0.4281\n",
      "Epoch 26/100, Train Loss: 2.5365, Validation Loss: 2.5030, Validation Accuracy: 0.4366\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e18d68373ca>\u001b[0m in \u001b[0;36m<cell line: 89>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-9e18d68373ca>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunning_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "seed = 20  \n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "net = resnet50()\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "net.to(device)\n",
    "\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0001, momentum=0.9)\n",
    "\n",
    "def train(net, dataloader, criterion, optimizer, device):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    return running_loss / (i + 1)\n",
    "\n",
    "\n",
    "def validate(net, dataloader, criterion, device):\n",
    "    net.eval()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return running_loss / (i + 1), correct / total\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "# val_accuracies = []\n",
    "\n",
    "# # Train the model\n",
    "# num_epochs = 100\n",
    "# for epoch in range(num_epochs):\n",
    "#     running_loss = 0.0\n",
    "#     for i, data in enumerate(train_loader, 0):\n",
    "#         inputs, labels = data\n",
    "#         inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         outputs = net(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#     test(net,validation_loader, device)\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / (i + 1)}\")\n",
    "\n",
    "# print(\"Finished Training\")\n",
    "\n",
    "# Train the model\n",
    "\n",
    "num_epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(net, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_accuracy = validate(net, validation_loader, criterion, device)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "print(\"Finished Training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Y0OTuR1XwO8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Loss vs. Epoch')\n",
    "\n",
    "# Plot the validation accuracy\n",
    "plt.figure()\n",
    "plt.plot(val_accuracies)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.title('Validation Accuracy vs. Epoch')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RmUciaKb8UIP"
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "\n",
    "net.eval()\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        images, labels = data\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100}%\")\n",
    "print(f\"F1 score: {f1}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "91nuqg2n-wax"
   },
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
